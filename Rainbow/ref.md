## Ref


Rainbow: Combining Improvements in Deep Reinforcement Learning<br>
https://arxiv.org/abs/1710.02298


Human-level control through deep reinforcement learning<br>
https://www.nature.com/articles/nature14236


Deep Reinforcement Learning with Double Q-learning<br>
https://arxiv.org/abs/1509.06461


Dueling Network Architectures for Deep Reinforcement Learning<br>
https://arxiv.org/abs/1511.06581


Noisy Networks for Exploration<br>
https://arxiv.org/abs/1706.10295


Prioritized Experience Replay<br>
https://arxiv.org/abs/1511.05952


A Distributional Perspective on Reinforcement Learning<br>
https://arxiv.org/abs/1707.06887



## Note

https://www.tensorflow.org/agents/tutorials/9_c51_tutorial

>Although C51 and n-step updates are often combined with prioritized replay to form the core of the Rainbow agent, we saw no measurable improvement from implementing prioritized replay. Moreover, we find that when combining our C51 agent with n-step updates alone, our agent performs as well as other Rainbow agents on the sample of Atari environments we've tested.



https://github.com/tensorflow/tensorflow/issues/26811
